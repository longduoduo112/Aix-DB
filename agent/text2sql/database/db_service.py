import warnings

from sqlalchemy import create_engine

from common.datasource_util import DatasourceConfigUtil, DatasourceConnectionUtil
from model import Datasource

warnings.filterwarnings("ignore", message=".*pkg_resources.*deprecated.*")

import hashlib
import json
import logging
import os
import re
import time
from functools import lru_cache
from typing import Dict, List, Tuple, Optional

import faiss
import jieba
import numpy as np
import pandas as pd
import requests

from langfuse.openai import OpenAI
from rank_bm25 import BM25Okapi
from sqlalchemy.inspection import inspect
from sqlalchemy.sql.expression import text

from agent.text2sql.state.agent_state import AgentState, ExecutionResult
from model.db_connection_pool import get_db_pool
from model.db_models import TAiModel

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥æ± 
db_pool = get_db_pool()

FORCE_REBUILD_VECTOR_INDEX = os.getenv("FORCE_REBUILD_VECTOR_INDEX", "false").lower() == "true"

# å‘é‡ç´¢å¼•å­˜å‚¨è·¯å¾„
VECTOR_INDEX_DIR = "./vector_index"
os.makedirs(VECTOR_INDEX_DIR, exist_ok=True)

INDEX_FILE = os.path.join(VECTOR_INDEX_DIR, "schema.index")
METADATA_FILE = os.path.join(VECTOR_INDEX_DIR, "metadata.json")


# åµŒå…¥æ¨¡å‹é…ç½®
def get_embedding_model_config():
    with db_pool.get_session() as session:
        # model_type: 2 -> Embedding
        model = session.query(TAiModel).filter(TAiModel.model_type == 2, TAiModel.default_model == True).first()

        if not model:
            # Fallback or raise error?
            # Trying to find ANY embedding model if default not set
            model = session.query(TAiModel).filter(TAiModel.model_type == 2).first()
        
        # Fallback to LLM
        if not model:
            model = session.query(TAiModel).filter(
                TAiModel.model_type == 1,
                TAiModel.default_model == True
            ).first()
            
        if not model:
             model = session.query(TAiModel).filter(TAiModel.model_type == 1).first()

        if not model:
            raise ValueError("æœªé…ç½®åµŒå…¥æ¨¡å‹ (Embedding Model) ä¸”æ— å¯ç”¨å¤§æ¨¡å‹")
            
        base_model = model.base_model
        if model.model_type == 1 and model.supplier == 1:
             base_model = "text-embedding-3-small"

        return {"name": base_model, "api_key": model.api_key, "base_url": model.api_domain}


# é‡æ’æ¨¡å‹é…ç½®
def get_rerank_model_config():
    with db_pool.get_session() as session:
        # model_type: 3 -> Rerank
        model = session.query(TAiModel).filter(TAiModel.model_type == 3, TAiModel.default_model == True).first()

        if not model:
            # Fallback
            model = session.query(TAiModel).filter(TAiModel.model_type == 3).first()

        if not model:
            return None

        return {"name": model.base_model, "api_key": model.api_key, "base_url": model.api_domain}


# å…¨å±€å˜é‡å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åŠ¨æ€è·å–æˆ–åœ¨ init ä¸­åˆå§‹åŒ–
# ä½†ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ lazy initialization æˆ–è€… property


class DatabaseService:
    """
    æ”¯æŒæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰ä¸ç´¢å¼•æŒä¹…åŒ–çš„æ•°æ®åº“æœåŠ¡ã€‚
    æä¾›è¡¨ç»“æ„æ£€ç´¢ã€SQL æ‰§è¡Œã€é”™è¯¯ä¿®æ­£ SQL æ‰§è¡Œç­‰åŠŸèƒ½ã€‚
    """

    def __init__(self, datasource_id: int = None):
        self._engine = None
        if datasource_id:
            try:
                with db_pool.get_session() as session:
                    ds = session.query(Datasource).filter(Datasource.id == datasource_id).first()
                    if ds:
                        config = DatasourceConfigUtil.decrypt_config(ds.configuration)
                        uri = DatasourceConnectionUtil.build_connection_uri(ds.type, config)
                        self._engine = create_engine(uri)
                        logger.info(f"Initialized DatabaseService with datasource_id: {datasource_id}")
            except Exception as e:
                logger.error(f"Failed to initialize datasource {datasource_id}: {e}")

        if not self._engine:
            self._engine = db_pool.get_engine()

        self._faiss_index: Optional[faiss.Index] = None
        self._table_names: List[str] = []
        self._corpus: List[str] = []
        self._tokenized_corpus: List[List[str]] = []
        self._index_initialized: bool = False
        self.USE_RERANKER: bool = True  # æ˜¯å¦å¯ç”¨é‡æ’åºå™¨

        # Initialize clients lazily or now
        try:
            emb_config = get_embedding_model_config()
            self.embedding_model_name = emb_config["name"]
            self.embedding_client = OpenAI(api_key=emb_config["api_key"] or "empty", base_url=emb_config["base_url"])
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            self.embedding_client = None

        try:
            rerank_config = get_rerank_model_config()
            if rerank_config:
                self.rerank_model_name = rerank_config["name"]
                self.rerank_api_key = rerank_config["api_key"]
                self.rerank_base_url = rerank_config["base_url"]
                self.USE_RERANKER = True
            else:
                self.USE_RERANKER = False
                logger.warning("æœªé…ç½®é‡æ’æ¨¡å‹ï¼Œé‡æ’åŠŸèƒ½å°†è¢«ç¦ç”¨")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–é‡æ’æ¨¡å‹å¤±è´¥: {e}")
            self.USE_RERANKER = False

    @staticmethod
    def _tokenize_text(text_str: str) -> List[str]:
        """
        å¯¹ä¸­æ–‡/è‹±æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè¿‡æ»¤æ ‡ç‚¹ç¬¦å·ã€‚
        """
        filtered_text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", " ", text_str)
        tokens = jieba.lcut(filtered_text, cut_all=False)
        return [token.strip() for token in tokens if token.strip()]

    def _get_table_comment(self, table_name: str) -> str:
        """
        ä» information_schema ä¸­è·å–æŒ‡å®šè¡¨çš„æ³¨é‡Šã€‚
        """
        try:
            query = text(
                """
                SELECT table_comment
                FROM information_schema.tables
                WHERE table_schema = DATABASE()
                  AND table_name = :table_name;
                """
            )
            with self._engine.connect() as conn:
                result = conn.execute(query, {"table_name": table_name})
                row = result.fetchone()
                return (row[0] or "").strip()
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} æ³¨é‡Šå¤±è´¥: {e}")
            return ""

    @staticmethod
    def _build_document(table_name: str, table_info: dict) -> str:
        """
        æ„å»ºç”¨äºæ£€ç´¢çš„æ–‡æ¡£æ–‡æœ¬ï¼ˆè¡¨å + æ³¨é‡Š + å­—æ®µå + å­—æ®µæ³¨é‡Šï¼‰ã€‚
        """
        parts = [table_name]
        if table_info.get("table_comment"):
            parts.append(table_info["table_comment"])
        for col_name, col_info in table_info.get("columns", {}).items():
            parts.append(col_name)
            if col_info.get("comment"):
                parts.append(col_info["comment"])
        return " ".join(parts)

    @lru_cache(maxsize=1)
    def _fetch_all_table_info(self) -> Dict[str, Dict]:
        """
        è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆå¸¦ LRU ç¼“å­˜ï¼‰ã€‚
        """
        start_time = time.time()
        inspector = inspect(self._engine)
        table_names = inspector.get_table_names()
        logger.info(f"ğŸ” å¼€å§‹åŠ è½½ {len(table_names)} å¼ è¡¨çš„ schema ä¿¡æ¯...")

        table_info = {}
        for table_name in table_names:
            try:
                columns = {}
                for col in inspector.get_columns(table_name):
                    columns[col["name"]] = {
                        "type": str(col["type"]),
                        "comment": str(col["comment"] or ""),
                    }

                foreign_keys = [
                    f"{fk['constrained_columns'][0]} -> {fk['referred_table']}.{fk['referred_columns'][0]}"
                    for fk in inspector.get_foreign_keys(table_name)
                ]

                table_comment = self._get_table_comment(table_name)

                table_info[table_name] = {
                    "columns": columns,
                    "foreign_keys": foreign_keys,
                    "table_comment": table_comment,
                }
            except Exception as e:
                logger.error(f"âŒ è¯»å–è¡¨ {table_name} ç»“æ„å¤±è´¥: {e}")

        elapsed = time.time() - start_time
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(table_info)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        return table_info

    @staticmethod
    def _generate_schema_fingerprint(table_info: Dict[str, Dict]) -> str:
        """
        ç”Ÿæˆ schema çš„æŒ‡çº¹ï¼ˆMD5 å“ˆå¸Œï¼‰ï¼Œç”¨äºæ£€æµ‹å˜æ›´ã€‚
        """
        fingerprint_data = {}
        for table_name, info in table_info.items():
            fingerprint_data[table_name] = {
                "comment": info.get("table_comment", ""),
                "columns": sorted(
                    [f"{col_name}:{col_info.get('comment', '')}" for col_name, col_info in info["columns"].items()]
                ),
            }
        json_str = json.dumps(fingerprint_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(json_str.encode("utf-8")).hexdigest()

    def _load_vector_index(self, table_info: Dict[str, Dict]) -> bool:
        """
        ä»ç£ç›˜åŠ è½½ FAISS å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®ã€‚
        """
        if not (os.path.exists(INDEX_FILE) and os.path.exists(METADATA_FILE)):
            logger.info("âŒ å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é‡å»º")
            return False

        try:
            with open(METADATA_FILE, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            current_fingerprint = self._generate_schema_fingerprint(table_info)
            if metadata.get("fingerprint") != current_fingerprint:
                logger.info("ğŸ”„ æ•°æ®åº“ schema å·²å˜æ›´ï¼Œéœ€é‡å»ºå‘é‡ç´¢å¼•")
                return False

            self._faiss_index = faiss.read_index(INDEX_FILE)
            self._table_names = metadata["table_names"]
            self._corpus = metadata["corpus"]

            logger.info(f"ğŸ‰ æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(self._table_names)} å¼ è¡¨")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†é‡å»º")
            return False

    def _save_vector_index(self, table_info: Dict[str, Dict]):
        """
        å°† FAISS ç´¢å¼•å’Œå…ƒæ•°æ®ä¿å­˜åˆ°ç£ç›˜ã€‚
        """
        if self._faiss_index is None:
            return

        faiss.write_index(self._faiss_index, INDEX_FILE)

        metadata = {
            "table_names": self._table_names,
            "corpus": self._corpus,
            "fingerprint": self._generate_schema_fingerprint(table_info),
            "updated_at": pd.Timestamp.now().isoformat(),
        }
        with open(METADATA_FILE, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜è‡³: {INDEX_FILE}")

    def _create_embeddings_with_dashscope(self, texts: List[str]) -> np.ndarray:
        """
        ä½¿ç”¨ DashScope API ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚
        """
        if not self.embedding_client:
            logger.error("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            return np.array([])

        logger.info(f"ğŸŒ è°ƒç”¨åµŒå…¥æ¨¡å‹ {self.embedding_model_name}...")
        start_time = time.time()
        embeddings = []
        for doc in texts:
            try:
                response = self.embedding_client.embeddings.create(model=self.embedding_model_name, input=doc)
                embeddings.append(response.data[0].embedding)
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥ ({doc[:30]}...): {e}")
                embeddings.append(np.zeros(1024))  # å ä½ç¬¦

        embeddings = np.array(embeddings).astype("float32")
        faiss.normalize_L2(embeddings)
        logger.info(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
        return embeddings

    def _initialize_vector_index(self, table_info: Dict[str, Dict]):
        """
        åˆå§‹åŒ– FAISS å‘é‡ç´¢å¼•ï¼šåŠ è½½æˆ–é‡å»ºã€‚
        """
        if self._index_initialized:
            return

        if FORCE_REBUILD_VECTOR_INDEX:
            logger.info("ğŸ’¡ å¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•ï¼ˆç¯å¢ƒå˜é‡è§¦å‘ï¼‰")
        elif self._load_vector_index(table_info):
            self._index_initialized = True
            return

        # æ„å»ºæ–°ç´¢å¼•
        logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºæ–°çš„å‘é‡ç´¢å¼•...")
        start_time = time.time()

        self._table_names = list(table_info.keys())
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]

        # ç”ŸæˆåµŒå…¥
        embeddings = self._create_embeddings_with_dashscope(self._corpus)

        if embeddings.size == 0:
            logger.error("âŒ æ— æ³•ç”ŸæˆåµŒå…¥ï¼Œç´¢å¼•æ„å»ºå¤±è´¥")
            return

        # åˆå§‹åŒ– FAISS ç´¢å¼•
        dimension = embeddings.shape[1]
        self._faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦
        self._faiss_index.add(embeddings)

        # ä¿å­˜ç´¢å¼•
        self._save_vector_index(table_info)

        elapsed = time.time() - start_time
        logger.info(f"ğŸ‰ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self._table_names)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        self._index_initialized = True

    def _retrieve_by_vector(self, query: str, top_k: int = 10) -> List[int]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸å…³çš„è¡¨ã€‚
        """
        if not self.embedding_client or not self._faiss_index:
            logger.error("âŒ å‘é‡æ£€ç´¢æœåŠ¡ä¸å¯ç”¨")
            return []

        try:
            response = self.embedding_client.embeddings.create(model=self.embedding_model_name, input=query)
            query_vec = np.array([response.data[0].embedding]).astype("float32")
            faiss.normalize_L2(query_vec)
            _, indices = self._faiss_index.search(query_vec, top_k)
            return indices[0].tolist()
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _retrieve_by_bm25(self, table_info: Dict[str, Dict], user_query: str) -> List[int]:
        """
        ä½¿ç”¨ BM25 ç®—æ³•è¿›è¡Œå…³é”®è¯åŒ¹é…æ£€ç´¢ã€‚
        """
        if not user_query or not table_info:
            return list(range(len(table_info)))

        logger.info("ğŸ”„ æ‰§è¡Œ BM25 æ£€ç´¢...")
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]
        self._tokenized_corpus = [self._tokenize_text(doc) for doc in self._corpus]
        query_tokens = self._tokenize_text(user_query)

        bm25 = BM25Okapi(self._tokenized_corpus)
        doc_scores = bm25.get_scores(query_tokens)

        # å¢å¼ºï¼šè‹¥æŸ¥è¯¢è¯å‡ºç°åœ¨è¡¨æ³¨é‡Šä¸­ï¼Œåˆ™æå‡åˆ†æ•°
        enhanced_scores = doc_scores.copy()
        table_comments = [info.get("table_comment", "") for info in table_info.values()]
        for i, (comment, score) in enumerate(zip(table_comments, doc_scores)):
            if score <= 0:
                continue
            comment_tokens = self._tokenize_text(comment)
            overlap = set(query_tokens) & set(comment_tokens)
            if overlap:
                overlap_ratio = len(overlap) / len(set(query_tokens))
                enhanced_scores[i] += score * overlap_ratio * 1.5

        scored_indices = sorted(enumerate(enhanced_scores), key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in scored_indices]

    @staticmethod
    def _rrf_fusion(bm25_indices: List[int], vector_indices: List[int], k: int = 60) -> List[int]:
        """
        ä½¿ç”¨ RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆä¸¤ç§æ£€ç´¢ç»“æœã€‚
        """
        scores = {}
        for rank, idx in enumerate(bm25_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        for rank, idx in enumerate(vector_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        sorted_indices = sorted(scores.items(), key=lambda x: -x[1])
        return [idx for idx, _ in sorted_indices]

    def _rerank_with_dashscope(self, query: str, candidate_tables: Dict[str, Dict]) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ DashScope é‡æ’ API å¯¹å€™é€‰è¡¨è¿›è¡Œé‡æ’åºã€‚
        """
        if not self.USE_RERANKER:
            logger.debug("â­ï¸ Reranker å·²ç¦ç”¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é‡æ’åº")
            return [(name, 1.0) for name in candidate_tables.keys()]

        try:
            documents = []
            name_to_text = {}
            for table_name, info in candidate_tables.items():
                doc_text = self._build_document(table_name, info)
                documents.append(doc_text)
                name_to_text[table_name] = doc_text

            if not documents:
                return []

            logger.info(f"ğŸ” è°ƒç”¨é‡æ’æ¨¡å‹ {self.rerank_model_name} è¿›è¡Œé‡æ’åº...")

            # æ ¹æ®APIç±»å‹é€‰æ‹©ä¸åŒçš„è¯·æ±‚ç»“æ„
            if "aliyuncs" in self.rerank_base_url or "Qwen" in self.rerank_model_name:
                # é˜¿é‡Œäº‘ DashScope æ ¼å¼
                payload = {
                    "model": self.rerank_model_name,
                    "input": {"query": query, "documents": documents},
                    "parameters": {"top_n": len(documents), "return_documents": False},
                }
            else:
                # å…¶ä»–æ ¼å¼ï¼ˆå¦‚æœ¬åœ°æ¨¡å‹æˆ–é€šç”¨rerank APIï¼‰
                payload = {"query": query, "documents": documents}

            # è®¾ç½®è¯·æ±‚å¤´
            headers = {"Authorization": f"Bearer {self.rerank_api_key}", "Content-Type": "application/json"}

            # è°ƒç”¨é‡æ’ API
            response = requests.post(self.rerank_base_url, headers=headers, json=payload, timeout=30)

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                logger.warning(f"âš ï¸ Rerank API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return [(name, 1.0) for name in candidate_tables.keys()]

            # è§£æå“åº”
            result_data = response.json()

            # æ ¹æ®APIç±»å‹è§£æå“åº”
            if "aliyuncs" in self.rerank_base_url or "Qwen" in self.rerank_model_name:
                # é˜¿é‡Œäº‘æ ¼å¼å“åº”
                if "output" in result_data and "results" in result_data["output"]:
                    results = []
                    for item in result_data["output"]["results"]:
                        idx = item["index"]
                        score = item["relevance_score"]
                        table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                        results.append((table_name, score))

                    results.sort(key=lambda x: x[1], reverse=True)
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results
            else:
                # é€šç”¨æ ¼å¼å“åº” - å‡è®¾ç›´æ¥è¿”å›æ’åºç»“æœ
                if "results" in result_data:
                    results = []
                    for item in result_data["results"]:
                        if "index" in item and "relevance_score" in item:  # ä½¿ç”¨relevance_score
                            idx = item["index"]
                            score = item["relevance_score"]  # ä½¿ç”¨relevance_scoreå­—æ®µ
                            # ä»documentå¯¹è±¡ä¸­æå–æ–‡æœ¬
                            if "document" in item and "text" in item["document"]:
                                doc_text = item["document"]["text"]
                                table_name = next(name for name, text in name_to_text.items() if text == doc_text)
                            else:
                                table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                            results.append((table_name, score))
                    results.sort(key=lambda x: x[1], reverse=True)
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results
                elif isinstance(result_data, list):
                    # å‡è®¾ç›´æ¥è¿”å›äº†æ’åºåçš„ç´¢å¼•åˆ—è¡¨
                    results = []
                    for i, item in enumerate(result_data):
                        if isinstance(item, dict) and "index" in item:
                            idx = item["index"]
                            score = item.get("score", 1.0 - i * 0.01)  # é»˜è®¤åˆ†æ•°é€’å‡
                            table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                            results.append((table_name, score))
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results

            logger.warning("âš ï¸ Rerank API è¿”å›æ ¼å¼å¼‚å¸¸")
            return [(name, 1.0) for name in candidate_tables.keys()]

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Rerank API è¯·æ±‚å¤±è´¥: {e}")
            return [(name, 1.0) for name in candidate_tables.keys()]
        except Exception as e:
            logger.error(f"âŒ Rerank è¿‡ç¨‹å‡ºé”™: {e}")
            return [(name, 1.0) for name in candidate_tables.keys()]

    def get_table_schema(self, state: AgentState) -> AgentState:
        """
        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œé€šè¿‡æ··åˆæ£€ç´¢ç­›é€‰å‡ºæœ€ç›¸å…³çš„æ•°æ®åº“è¡¨ç»“æ„ã€‚
        """
        try:
            logger.info("ğŸ” å¼€å§‹è·å–æ•°æ®åº“è¡¨ schema ä¿¡æ¯")
            all_table_info = self._fetch_all_table_info()

            user_query = state.get("user_query", "").strip()
            if not user_query:
                state["db_info"] = all_table_info
                logger.info(f"â„¹ï¸ æ— ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›å…¨éƒ¨ {len(all_table_info)} å¼ è¡¨")
                return state

            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            self._initialize_vector_index(all_table_info)

            # æ··åˆæ£€ç´¢
            logger.info("ğŸ” å¼€å§‹æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡æ£€ç´¢")
            bm25_top_indices = self._retrieve_by_bm25(all_table_info, user_query)
            logger.info(f"ğŸ“Š BM25æ£€ç´¢è¿”å› {len(bm25_top_indices)} ä¸ªç»“æœ")
            vector_top_indices = self._retrieve_by_vector(user_query, top_k=20)
            logger.info(f"ğŸ”— å‘é‡æ£€ç´¢è¿”å› {len(vector_top_indices)} ä¸ªç»“æœ")

            # è¿‡æ»¤ï¼šä»…ä¿ç•™åŒæ—¶åœ¨ BM25 å‰ 50 å’Œå‘é‡ç»“æœä¸­çš„è¡¨
            valid_bm25_set = set(bm25_top_indices[:50])
            candidate_indices = [idx for idx in vector_top_indices if idx in valid_bm25_set]
            logger.info(f"ğŸ¯ åˆæ­¥ç­›é€‰åä¿ç•™ {len(candidate_indices)} ä¸ªå€™é€‰è¡¨")

            if not candidate_indices:
                candidate_indices = bm25_top_indices[:4]  # é™çº§
                logger.info("âš ï¸ å€™é€‰è¡¨ä¸ºç©ºï¼Œé™çº§ä½¿ç”¨BM25å‰4ä¸ªç»“æœ")

            fused_indices = self._rrf_fusion(bm25_top_indices, candidate_indices, k=60)
            logger.info(f"ğŸ”„ RRFèåˆåå¾—åˆ° {len(fused_indices)} ä¸ªç»“æœ")

            # è¯„åˆ†ç­›é€‰
            selected_indices = []
            for idx in fused_indices:
                bm25_rank = bm25_top_indices.index(idx) + 1 if idx in bm25_top_indices else len(all_table_info) + 1
                vector_rank = (
                    vector_top_indices.index(idx) + 1 if idx in vector_top_indices else len(all_table_info) + 1
                )
                score = 1 / (60 + bm25_rank) + 1 / (60 + vector_rank)
                if score >= 0.01 and len(selected_indices) < 10:
                    selected_indices.append(idx)

            candidate_table_names = [self._table_names[i] for i in selected_indices]
            candidate_table_info = {name: all_table_info[name] for name in candidate_table_names}

            # é‡æ’åº
            reranked_results = self._rerank_with_dashscope(user_query, candidate_table_info)
            final_table_names = [name for name, _ in reranked_results][:4]  # å– top 4

            # æ„å»ºè¾“å‡º
            filtered_info = {name: all_table_info[name] for name in final_table_names}

            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            print("ğŸ“Š æ£€ç´¢ä¸æ’åºç»“æœ:")
            for i, (table_name, score) in enumerate(reranked_results):
                bm25_idx = self._table_names.index(table_name) if table_name in self._table_names else -1
                bm25_rank = bm25_top_indices.index(bm25_idx) + 1 if bm25_idx in bm25_top_indices else "-"
                vector_rank = vector_top_indices.index(bm25_idx) + 1 if bm25_idx in vector_top_indices else "-"
                print(
                    f"  {i + 1}. {table_name:<15} | BM25: {bm25_rank:>2} | Vector: {vector_rank:>2} | Rerank: {score:.3f}"
                )

            state["db_info"] = filtered_info
            logger.info(f"âœ… æœ€ç»ˆç­›é€‰å‡º {len(filtered_info)} ä¸ªç›¸å…³è¡¨: {list(filtered_info.keys())}")

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®åº“è¡¨ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            state["db_info"] = {}
            state["execution_result"] = ExecutionResult(success=False, error="æ— æ³•è¿æ¥æ•°æ®åº“æˆ–è·å–å…ƒæ•°æ®")

        return state

    def execute_sql(self, state: AgentState) -> AgentState:
        """
        æ‰§è¡Œç”Ÿæˆçš„ SQL è¯­å¥ã€‚
        """
        generated_sql = state.get("generated_sql", "").strip()
        if not generated_sql:
            error_msg = "SQL ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œ"
            logger.warning(error_msg)
            state["execution_result"] = ExecutionResult(success=False, error=error_msg)
            return state

        logger.info("â–¶ï¸ æ‰§è¡Œ SQL è¯­å¥")
        try:
            with self._engine.connect() as connection:
                result = connection.execute(text(generated_sql))
                result_data = result.fetchall()
                columns = result.keys()
                frame = pd.DataFrame(result_data, columns=columns)
                state["execution_result"] = ExecutionResult(success=True, data=frame.to_dict(orient="records"))
                logger.info(f"âœ… SQL æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result_data)} æ¡è®°å½•")
        except Exception as e:
            error_msg = f"æ‰§è¡Œ SQL å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            state["execution_result"] = ExecutionResult(success=False, error=str(e))
        return state
